\input{cs440_style.tex}
\usepackage{algorithm}
\usepackage{listings}
%\usepackage{algpseudocode}
\usepackage{graphicx,amssymb,amsmath}
\usepackage{epstopdf}
\usepackage{color}
\sloppy


\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in

\begin{document}

\solution{Dan McQuillan}{\today}{2 - Part 2 Problem 3}{Spring 2014}

\pagestyle{myheadings}  % Leave this command alone

\begin{enumerate}

	\item{\bf Solution to problem 3}
		\begin{enumerate}
			\item{Find and report the list MAX for Dataset 2} \\
				\( (70.0, 1.0, 4.0, 170.0, 407.0, 1.0, 2.0, 194.0, 1.0, 4.0, 3.0, 3.0, 7.0) \)
			\item{Repeat Problem 1} \\ \\
				\bf{Training Dataset 1x} \\
				\noindent\rule{8cm}{0.4pt} \\
				\textnormal{The training took 14 epochs.} \\
				\(Threshhold: 5.0 \) \\
				\( \gamma \rightarrow 0.0320963696907362 \) \\
				\( w_{1} \rightarrow -1.2368421052631573 \) \\
				\( w_{2} \rightarrow 3.0 \) \\
				\( w_{3} \rightarrow 5.5 \) \\
				\( w_{4} \rightarrow -0.2899999999999997 \) \\
				\( w_{5} \rightarrow -0.13829787234042568 \) \\
				\( w_{6} \rightarrow 0.0 \) \\
				\( w_{7} \rightarrow 0.5 \) \\
				\( w_{8} \rightarrow -13.317204301075275 \) \\
				\( w_{9} \rightarrow 3.0 \) \\
				\( w_{10} \rightarrow 5.645161290322578 \) \\
				\( w_{11} \rightarrow 2.9999999999999987 \) \\
				\( w_{12} \rightarrow 5.0 \) \\
				\( w_{13} \rightarrow 5.7142857142857135 \) \\

				\bf{Test Dataset 1x} \\
				\noindent\rule{8cm}{0.4pt} \\
				\textnormal{Confusion Matrix:} \\ \\
				\(
					\begin{array}{|c|c|}
						\hline
							54 & 0 \\
							\hline
							0 & 63 \\
						\hline
					\end{array}
				\) \\ \\
				\textnormal{Total loss: 0.0} \\
				
				\bf{Test Dataset 2x} \\
				\noindent\rule{8cm}{0.4pt} \\
				\textnormal{False Positives:} \\
				\textnormal{Index: 7 }\\
				\textnormal{ Confusion matrix: } \\ \\
				\(
					\begin{array}{|c|c|}
						\hline
							13 & 0 \\
							\hline
							1 & 19 \\
						\hline
					\end{array}
				\) \\ \\
				\textnormal{Total loss:} \(0.010623925503195387\) \\
				
				\bf{Application Dataset 3x} \\
				\noindent\rule{8cm}{0.4pt} \\
				\( 1 \rightarrow 0.0 \) \\
				\( 2 \rightarrow 1.0 \) \\
				\( 3 \rightarrow 0.0 \) \\
				\( 4 \rightarrow 1.0 \) \\
				\( 5 \rightarrow 1.0 \) \\
				\( 6 \rightarrow 0.0 \) \\
				\( 7 \rightarrow 0.0 \) \\
				\( 8 \rightarrow 0.0 \) \\
				\( 9 \rightarrow 0.0 \) \\
				\( 10 \rightarrow 1.0 \) \\
				\( 11 \rightarrow 1.0 \) \\
				\( 12 \rightarrow 0.0 \) \\
				\( 13 \rightarrow 1.0 \) \\
				\( 14 \rightarrow 0.0 \) \\
				\( 15 \rightarrow 0.0 \) \\
				\( 16 \rightarrow 0.0 \) \\
				\( 17 \rightarrow 1.0 \) \\
				\( 18 \rightarrow 0.0 \) \\
				\( 19 \rightarrow 0.0 \) \\
				\( 20 \rightarrow 0.0 \) \\
				\( 21 \rightarrow 0.0 \) \\
				
			\item{Explanation} \\
				\textnormal{The results above show that the number of epoch were quite a bit less. This is a result of the perceptron having to oscillate because of the larger difference between input vectors. Instead it is more likely to increase with a smaller \(\Delta w\) and have a lesser chance to have to use a negative  \(\Delta w\) to correct a movement when it moves past the margin of a ``good'' perceptron.}
			\item{Test new perceptron on DataSet1} \\
				\textnormal{When testing data set 1 it resulted in a complete failure of classifying the input vectors from dataset one.  This is a result of normalizing the input vectors before training which caused the weight vector to be much smaller.  Although we normalized the input vectors that does not correlate to the weight vector itself being normalized. } \\ \\
			\item{Is is possible to recover the problem 1 perceptron} \\
				\textnormal{It is not possible to recover the perceptron from problem 1 from this weight vector since the resultant weight vectors are dependent on the values chosen to be the initial training weights.  It will not also be able to be recovered since the test on DataSet1 failed which means that the resultant vector from part d is not the weight vector normalized but instead it has a different direction entirely.  Since it lacks this corollary it is not possible to recover the perceptron from problem 1.  In addition, even if we used the resultant weight vector as the initialization of the perceptron, then trained the weight vector we would still be left with a different perceptron.}
				
			
		\end{enumerate}
\end{enumerate}
\end{document}

