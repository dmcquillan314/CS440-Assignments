\input{cs440_style.tex}
\usepackage{algorithm}
\usepackage{listings}
%\usepackage{algpseudocode}
\usepackage{graphicx,amssymb,amsmath}
\usepackage{epstopdf}

\usepackage{color}
\usepackage{listings}
\lstset{ %
language=Java,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}

\sloppy


\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in

\begin{document}

\solution{Dan McQuillan}{\today}{4 - Problem 2}{Spring 2014}

\pagestyle{myheadings}  % Leave this command alone

\begin{enumerate}

	\item {\bf Solution to problem 2}
	
		\begin{enumerate}
		
			\item[(a)] Explain how kernel functions affect the SVM
			
			The degree of the polynomial kernel and the phi parameter of the Gaussian kernel control the flexibility of the resulting SVM in fitting the data. This can affect the speed of convergence of the SVM's training as well.
			
			\item[(b)] Why would some kernel functions perform better than others?
			
			If the complexity is too large (degree too high, wrong phi value for Gaussian) then over fitting will occur since the decision boundary will only perform well on the training data and will lose generality since the decision boundary could eventually become the dataset with a high enough complexity.
			
			\item[(c)] Train the classifier for each of the following kernel functions
			
				\(
					\begin{array}{l|cc}
					
						\text{Kernel Function} & \text{Time taken(s)} & \text{Accuracy} \\
						\hline
						\text{ \emph{i} Linear or string kernel } & 0.826 & 60.2803738317757\% \\
						\text{ \emph{ii} Homogeneous quadratic kernel } & 0.325 & 61.44859813084112\% \\
						\text{ \emph{iii} Nonhomogeneous quadratic kernel } & 0.293 & 62.77258566978193\% \\
						\text{ \emph{iiv} Homogeneous cubic kernel } & 0.353 & 63.43457943925234\% \\
					
					\end{array}
				\)
			
		\end{enumerate}	 	

\end{enumerate}

\section{Source code:}

\subsection{MainP2.java}

\begin{lstlisting}
package hw4.weka;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.Random;

import weka.classifiers.evaluation.Evaluation;
import weka.classifiers.evaluation.output.prediction.XML;
import weka.classifiers.functions.SMO;
import weka.classifiers.functions.supportVector.Kernel;
import weka.classifiers.functions.supportVector.PolyKernel;
import weka.core.Instances;

public class MainP2 {
		
	public static PolyKernel buildKernel(final Integer cacheSize, final Double exponent, final Boolean useLowerOrder, final Instances instances ) throws Exception {
		PolyKernel polyKernel = new PolyKernel();
		
		// Linear Kernel
		polyKernel.setExponent( exponent );
		polyKernel.setCacheSize( cacheSize );
		polyKernel.setUseLowerOrder( useLowerOrder );
		polyKernel.buildKernel( instances );
		
		return polyKernel;
	}
	
	public static void main(String[] args)  {
		try {
			File file = new File("glass.arff");
			BufferedReader bufferedReader = new BufferedReader(new FileReader(file));
			
			
			Instances instances = new Instances(bufferedReader);
			
			bufferedReader.close();
			
			final int numInstances = instances.numAttributes();			
			instances.setClassIndex(numInstances - 1);
			
			// Part c			
			Instances trainInstances = instances;
			final Integer folds = 10;

			StringBuilder applicationOutput = new StringBuilder();

			applicationOutput.append("Begin problem 2 part c\n");
			applicationOutput.append("folds: " + folds + "\n\n");

				
			StringBuffer internalStringBuffer = new StringBuffer();
			XML internalOutput = new XML();
			internalOutput.setBuffer(internalStringBuffer);
			internalOutput.setHeader(trainInstances);
			internalOutput.setOutputDistribution(true);

			Evaluation evaluation = new Evaluation(trainInstances);
			
			PolyKernel linearKernel = buildKernel(0, 1.0, false, trainInstances);
			PolyKernel homogeneousQuadraticKernel = buildKernel(0, 2.0, false, trainInstances);
			PolyKernel nonhomogeneousQuadraticKernel = buildKernel(0, 2.0, true, trainInstances);
			PolyKernel homogeneousCubickernel = buildKernel(0, 3.0, false, trainInstances);
			
			List<PolyKernel> kernels = new ArrayList<PolyKernel>();
			kernels.add(linearKernel);
			kernels.add(homogeneousQuadraticKernel);
			kernels.add(nonhomogeneousQuadraticKernel);
			kernels.add(homogeneousCubickernel);
			
			for( Kernel kernel : kernels ) {
				
				applicationOutput.append( "Evaluating using kernel function: " + kernel.toString() + "\n" );
				
				SMO classifier = new SMO();
				classifier.setKernel(kernel);
				Date beforeCrossValidate = new Date();
				evaluation.crossValidateModel(classifier, trainInstances, folds, new Random( new Date().getTime() ), internalOutput );
				Date afterCrossValidate = new Date();
				
				Double timeTakenCrossValidation =  ((double)( afterCrossValidate.getTime() - beforeCrossValidate.getTime())) / 1000.0;
				applicationOutput.append("Time taken: " + timeTakenCrossValidation + "\n");	
				applicationOutput.append( "Accurracy: " + evaluation.pctCorrect() + "%\n\n");
			}
			
			System.out.println(applicationOutput.toString());
		} catch (IOException e) {
			System.err.println( e );
		} catch (Exception e) {
			e.printStackTrace();
		}
			
	}
}
\end{lstlisting}

\end{document}

